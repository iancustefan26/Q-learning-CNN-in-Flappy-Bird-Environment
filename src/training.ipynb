{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3291abbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefaniancu/Documents/Proiecte/Q-Learning CNN Flappy Bird/.venv/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets # for Mist\n",
    "import torchvision.transforms as transforms # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn # To inherit our neural network\n",
    "from torch.utils.data import DataLoader # For management of the dataset (batches)\n",
    "from tqdm import tqdm # For nice progress bar!\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import flappy_bird_gymnasium as flappy_bird\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from preprocessing.preprocessing import preprocess_frame\n",
    "from PIL import Image\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from static_variables import CHECKPOINTS_DIR, VIDEOS_DIR, PHOTOS_DIR, make_dirs\n",
    "import os\n",
    "import hashlib\n",
    "from preprocessing.preprocessing import preprocess_frame\n",
    "from model import DQN_CNN\n",
    "from save_model.utils import save_model, load_model, record_trained_agent_video\n",
    "from replay_buffer.ReplayBuffer import ReplayBuffer\n",
    "from model import save_input_frames\n",
    "from save_model.utils import transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367732a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe492045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = torch.device(device)\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40f957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "frame_skip = 4\n",
    "\n",
    "# Training\n",
    "data_type = torch.float32\n",
    "batch_size = 64\n",
    "T_iterations = 50000   # very important\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer_size = 20000\n",
    "# DQN\n",
    "alpha = 1e-4\n",
    "gamma = 0.99\n",
    "\n",
    "# Target network (soft update)\n",
    "target_network_incorporation_rate = 0.005\n",
    "\n",
    "# Epsilon-greedy\n",
    "e_start = 1.0\n",
    "e_end = 0.01\n",
    "k_epsilon = -1 / T_iterations * np.log(e_end / e_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefaniancu/Documents/Proiecte/Q-Learning CNN Flappy Bird/.venv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/stefaniancu/Documents/Proiecte/Q-Learning CNN Flappy Bird/.venv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Reward: -0.70 | Epsilon: 0.9989 | Best Reward: -0.70\n",
      "Episode 1 | Reward: -2.50 | Epsilon: 0.9978 | Best Reward: -0.70\n",
      "Episode 2 | Reward: -3.70 | Epsilon: 0.9967 | Best Reward: -0.70\n",
      "Episode 3 | Reward: -1.30 | Epsilon: 0.9956 | Best Reward: -0.70\n",
      "Episode 4 | Reward: -1.30 | Epsilon: 0.9945 | Best Reward: -0.70\n",
      "Episode 5 | Reward: -1.30 | Epsilon: 0.9935 | Best Reward: -0.70\n",
      "Episode 6 | Reward: -1.30 | Epsilon: 0.9924 | Best Reward: -0.70\n",
      "Episode 7 | Reward: -1.30 | Epsilon: 0.9913 | Best Reward: -0.70\n",
      "Episode 8 | Reward: -1.30 | Epsilon: 0.9902 | Best Reward: -0.70\n",
      "Episode 9 | Reward: -1.30 | Epsilon: 0.9891 | Best Reward: -0.70\n",
      "Episode 10 | Reward: -1.70 | Epsilon: 0.9879 | Best Reward: -0.70\n",
      "Episode 11 | Reward: -1.30 | Epsilon: 0.9868 | Best Reward: -0.70\n",
      "Episode 12 | Reward: -1.30 | Epsilon: 0.9857 | Best Reward: -0.70\n",
      "Episode 13 | Reward: -1.30 | Epsilon: 0.9846 | Best Reward: -0.70\n",
      "Episode 14 | Reward: -1.30 | Epsilon: 0.9835 | Best Reward: -0.70\n",
      "Episode 15 | Reward: -1.30 | Epsilon: 0.9825 | Best Reward: -0.70\n",
      "Episode 16 | Reward: -1.30 | Epsilon: 0.9814 | Best Reward: -0.70\n",
      "Episode 17 | Reward: -1.30 | Epsilon: 0.9803 | Best Reward: -0.70\n",
      "Episode 18 | Reward: -1.30 | Epsilon: 0.9792 | Best Reward: -0.70\n",
      "Episode 19 | Reward: -1.30 | Epsilon: 0.9782 | Best Reward: -0.70\n",
      "Episode 20 | Reward: -1.30 | Epsilon: 0.9771 | Best Reward: -0.70\n",
      "Episode 21 | Reward: -1.30 | Epsilon: 0.9760 | Best Reward: -0.70\n",
      "Episode 22 | Reward: -1.30 | Epsilon: 0.9750 | Best Reward: -0.70\n",
      "Episode 23 | Reward: -1.30 | Epsilon: 0.9739 | Best Reward: -0.70\n",
      "Episode 24 | Reward: -1.30 | Epsilon: 0.9728 | Best Reward: -0.70\n",
      "Episode 25 | Reward: -1.30 | Epsilon: 0.9718 | Best Reward: -0.70\n",
      "Episode 26 | Reward: -1.30 | Epsilon: 0.9707 | Best Reward: -0.70\n",
      "Episode 27 | Reward: -1.30 | Epsilon: 0.9697 | Best Reward: -0.70\n",
      "Episode 28 | Reward: -1.30 | Epsilon: 0.9686 | Best Reward: -0.70\n",
      "Episode 29 | Reward: -1.30 | Epsilon: 0.9675 | Best Reward: -0.70\n",
      "Episode 30 | Reward: -1.30 | Epsilon: 0.9665 | Best Reward: -0.70\n",
      "Episode 31 | Reward: -1.30 | Epsilon: 0.9654 | Best Reward: -0.70\n",
      "Episode 32 | Reward: -1.30 | Epsilon: 0.9644 | Best Reward: -0.70\n",
      "Episode 33 | Reward: 1.50 | Epsilon: 0.9631 | Best Reward: 1.50\n",
      "Episode 34 | Reward: -1.30 | Epsilon: 0.9621 | Best Reward: 1.50\n",
      "Episode 35 | Reward: -1.30 | Epsilon: 0.9610 | Best Reward: 1.50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "policy_net = DQN_CNN(frame_skip).to(device)\n",
    "target_net = DQN_CNN(frame_skip).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "memory = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "global_step = 0\n",
    "frame_id = 0\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "for episode in range(T_iterations):\n",
    "    env.reset()\n",
    "\n",
    "    # ---------- initialize frame stack ----------\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    current_state, _, _ = transition(0, env, frame_skip)\n",
    "\n",
    "    while not done:\n",
    "        global_step += 1\n",
    "        #print(\"Current state: \", current_state.shape, type(current_state), len(current_state))\n",
    "        # ---------- epsilon decay ----------\n",
    "        epsilon = e_end + (e_start - e_end) * np.exp(-k_epsilon * global_step)\n",
    "        \n",
    "        state_tensor = torch.tensor(\n",
    "             current_state, dtype=torch.float32\n",
    "        ).unsqueeze(0).to(device)\n",
    "        #print(\"State tensor shape: \", state_tensor.shape)\n",
    "\n",
    "        # ---------- epsilon-greedy ----------\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # if global_step % 100 == 0:\n",
    "                #     save_input_frames(\n",
    "                #         state_tensor.cpu(),\n",
    "                #         f\"vis/step_{global_step:06d}_input.png\"\n",
    "                #     )\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "\n",
    "        next_state, reward, done = transition(action, env, frame_skip)\n",
    "        memory.push(current_state, action, reward, next_state, done)\n",
    "\n",
    "        episode_reward += reward\n",
    "        current_state = next_state\n",
    "\n",
    "        # ---------- learning ----------\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(next_states).max(1, keepdim=True)[0]\n",
    "                target_q = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "            loss = nn.SmoothL1Loss()(q_values, target_q)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 10)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # ---------- soft target update ----------\n",
    "            for target_param, policy_param in zip(\n",
    "                target_net.parameters(), policy_net.parameters()\n",
    "            ):\n",
    "                target_param.data.copy_(\n",
    "                    target_network_incorporation_rate * policy_param.data\n",
    "                    + (1.0 - target_network_incorporation_rate) * target_param.data\n",
    "                )\n",
    "                \n",
    "    best_reward = max(best_reward, episode_reward)\n",
    "    print(\n",
    "        f\"Episode {episode} | Reward: {episode_reward:.2f} | Epsilon: {epsilon:.4f} | Best Reward: {best_reward:.2f} | Replay Buffer Size: {len(memory)}\"\n",
    "    )\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5478de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to ../checkpoints/0d3d127ae33c6fc76f0744bd4756046c\n"
     ]
    }
   ],
   "source": [
    "path = save_model(\n",
    "    policy_net,\n",
    "    target_net,\n",
    "    optimizer,\n",
    "    dir=CHECKPOINTS_DIR,\n",
    "    global_step=global_step,\n",
    "    best_reward=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ed1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefaniancu/Documents/Proiecte/Q-Learning CNN Flappy Bird/.venv/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/stefaniancu/Documents/Proiecte/Q-Learning CNN Flappy Bird/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/stefaniancu/Documents/Proiecte/Q-Learning CNN Flappy Bird/.venv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/stefaniancu/Documents/Proiecte/Q-Learning CNN Flappy Bird/.venv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ Video saved in '../videos/' | Reward: 0.20\n"
     ]
    }
   ],
   "source": [
    "record_trained_agent_video(\n",
    "    model_path=path,\n",
    "    video_dir=VIDEOS_DIR,\n",
    "    frame_skip=frame_skip,\n",
    "    device=device,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
