{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets # for Mist\n",
    "import torchvision.transforms as transforms # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn # To inherit our neural network\n",
    "from torch.utils.data import DataLoader # For management of the dataset (batches)\n",
    "from tqdm import tqdm # For nice progress bar!\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import flappy_bird_gymnasium as flappy_bird\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from preprocessing.preprocessing import preprocess_frame\n",
    "from PIL import Image\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from static.static_variables import make_dirs, CHECKPOINTS_DIR, VIDEOS_DIR, PHOTOS_DIR\n",
    "import os\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "367732a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_dirs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmake_dirs\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'make_dirs' is not defined"
     ]
    }
   ],
   "source": [
    "make_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe492045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = torch.device(device)\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "frame_skip = 3\n",
    "\n",
    "# Training\n",
    "data_type = torch.float32\n",
    "batch_size = 64\n",
    "T_iterations = 10000   # very important\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer_size = 150_000\n",
    "\n",
    "# DQN\n",
    "alpha = 1e-4\n",
    "gamma = 0.99\n",
    "\n",
    "# Target network (soft update)\n",
    "target_network_incorporation_rate = 0.005\n",
    "\n",
    "# Epsilon-greedy\n",
    "e_start = 1.0\n",
    "e_end = 0.05\n",
    "k_epsilon = -1 / T_iterations * np.log(e_end / e_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_CNN, self).__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "         #w 82 - 7 + 2 * 3 / 2 + 1 = 41\n",
    "\n",
    "         #w maxpool\n",
    "         #w 41 / 2 = 20 out\n",
    "\n",
    "         #h 136 - 7 + 2 * 3 / 2 + 1 = 68\n",
    "\n",
    "        #h maxpool\n",
    "        #h 68 / 2 = 34 out\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=frame_skip,\n",
    "            out_channels=16,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3,\n",
    "        )\n",
    "\n",
    "\n",
    "        #w maxpool\n",
    "        #w 20 / 2 = 10 out\n",
    "\n",
    "        #h maxpool\n",
    "        #h 34 / 2 = 17 out\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 10 * 17, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # Assuming 2 actions: flap or not flap\n",
    "\n",
    "        conv_layers = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.ReLU(),\n",
    "            self.pool,\n",
    "            self.conv2,\n",
    "            nn.ReLU(),\n",
    "            self.pool,\n",
    "        )\n",
    "\n",
    "        linear_layers = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            self.fc2    \n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            conv_layers,\n",
    "            nn.Flatten(),\n",
    "            linear_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e1a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'photos/frame_00000.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m frame_stack = deque(maxlen=frame_skip)\n\u001b[32m     20\u001b[39m frame = preprocess_frame(env.render())\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPHOTOS_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/frame_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mframe_id\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m05d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.png\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m frame_id += \u001b[32m1\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frame_skip):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Proiecte/Q-Learning CNN Flappy Bird/.venv/lib/python3.13/site-packages/PIL/Image.py:2583\u001b[39m, in \u001b[36mImage.save\u001b[39m\u001b[34m(self, fp, format, **params)\u001b[39m\n\u001b[32m   2581\u001b[39m         fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mr+b\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2582\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2583\u001b[39m         fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw+b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2585\u001b[39m     fp = cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'photos/frame_00000.png'"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "policy_net = DQN_CNN().to(device)\n",
    "target_net = DQN_CNN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "memory = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "global_step = 0\n",
    "frame_id = 0\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "for episode in range(T_iterations):\n",
    "    env.reset()\n",
    "\n",
    "    # ---------- initialize frame stack ----------\n",
    "    frame_stack = deque(maxlen=frame_skip)\n",
    "    frame = preprocess_frame(env.render())\n",
    "\n",
    "    Image.fromarray(frame).save(f\"{PHOTOS_DIR}/frame_{frame_id:05d}.png\")\n",
    "    frame_id += 1\n",
    "\n",
    "    for _ in range(frame_skip):\n",
    "        frame_stack.append(frame)\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        global_step += 1\n",
    "\n",
    "        # ---------- epsilon decay ----------\n",
    "        epsilon = e_end + (e_start - e_end) * np.exp(-k_epsilon * global_step)\n",
    "\n",
    "        state = np.stack(frame_stack, axis=0)  # (skip_frames, H, W)\n",
    "        state_tensor = torch.tensor(\n",
    "            state, dtype=torch.float32\n",
    "        ).unsqueeze(0).to(device)\n",
    "        #print(\"State tensor shape: \", state_tensor.shape)\n",
    "        # ---------- epsilon-greedy ----------\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "\n",
    "        # ---------- frame skipping ----------\n",
    "        total_reward = 0\n",
    "        for _ in range(frame_skip):\n",
    "            _, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # ---------- get next rendered frame ----------\n",
    "        next_frame = preprocess_frame(env.render())\n",
    "        frame_stack.append(next_frame)\n",
    "        next_state = np.stack(frame_stack, axis=0)\n",
    "\n",
    "        memory.push(state, action, total_reward, next_state, done)\n",
    "        episode_reward += total_reward\n",
    "\n",
    "        # ---------- learning ----------\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(next_states).max(1, keepdim=True)[0]\n",
    "                target_q = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "            loss = nn.MSELoss()(q_values, target_q)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ---------- soft target update ----------\n",
    "            for target_param, policy_param in zip(\n",
    "                target_net.parameters(), policy_net.parameters()\n",
    "            ):\n",
    "                target_param.data.copy_(\n",
    "                    target_network_incorporation_rate * policy_param.data\n",
    "                    + (1.0 - target_network_incorporation_rate) * target_param.data\n",
    "                )\n",
    "    best_reward = max(best_reward, episode_reward)\n",
    "    print(\n",
    "        f\"Episode {episode} | Reward: {episode_reward:.2f} | Epsilon: {epsilon:.4f} | Best Reward: {best_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(\n",
    "    policy_net,\n",
    "    target_net,\n",
    "    optimizer,\n",
    "    filepath,\n",
    "    global_step=None,\n",
    "    best_reward=None,\n",
    "):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"policy_net_state_dict\": policy_net.state_dict(),\n",
    "        \"target_net_state_dict\": target_net.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"global_step\": global_step,\n",
    "        \"best_reward\": best_reward,\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"âœ… Model saved to {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5478de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to checkpoints/flappy_dqn.pt\n"
     ]
    }
   ],
   "source": [
    "save_model(\n",
    "    policy_net,\n",
    "    target_net,\n",
    "    optimizer,\n",
    "    filepath=\"checkpoints/flappy_dqn.pt\",\n",
    "    global_step=global_step,\n",
    "    best_reward=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e40160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    policy_net,\n",
    "    target_net,\n",
    "    optimizer,\n",
    "    filepath,\n",
    "    device,\n",
    "):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "\n",
    "    policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
    "    target_net.load_state_dict(checkpoint[\"target_net_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    policy_net.to(device)\n",
    "    target_net.to(device)\n",
    "\n",
    "    global_step = checkpoint.get(\"global_step\", 0)\n",
    "    best_reward = checkpoint.get(\"best_reward\", None)\n",
    "\n",
    "    print(f\"âœ… Model loaded from {filepath}\")\n",
    "\n",
    "    return global_step, best_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01501060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded from checkpoints/flappy_dqn.pt\n"
     ]
    }
   ],
   "source": [
    "policy_net = DQN_CNN().to(device)\n",
    "target_net = DQN_CNN().to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "global_step, best_reward = load_model(\n",
    "    policy_net,\n",
    "    target_net,\n",
    "    optimizer,\n",
    "    filepath=\"checkpoints/flappy_dqn.pt\",\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7fda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_trained_agent_video(\n",
    "    model_path=\"checkpoints/flappy_dqn.pt\",\n",
    "    video_dir=\"videos\",\n",
    "    env_name=\"FlappyBird-v0\",\n",
    "    frame_skip=4,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    # ---------- environment with video recording ----------\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    env = RecordVideo(\n",
    "        env,\n",
    "        video_folder=video_dir,\n",
    "        episode_trigger=lambda episode_id: True,  # record first episode\n",
    "        name_prefix=\"flappy_dqn\",\n",
    "    )\n",
    "\n",
    "    # ---------- load model ----------\n",
    "    policy_net = DQN_CNN().to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
    "    policy_net.eval()\n",
    "\n",
    "    # ---------- reset ----------\n",
    "    env.reset()\n",
    "    frame_stack = deque(maxlen=frame_skip)\n",
    "\n",
    "    frame = preprocess_frame(env.render())\n",
    "    for _ in range(frame_skip):\n",
    "        frame_stack.append(frame)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    # ---------- play episode ----------\n",
    "    while not done:\n",
    "        state = np.stack(frame_stack, axis=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            action = policy_net(state_t).argmax(dim=1).item()\n",
    "\n",
    "        reward_sum = 0\n",
    "        for _ in range(frame_skip):\n",
    "            _, reward, terminated, truncated, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        frame = preprocess_frame(env.render())\n",
    "        frame_stack.append(frame)\n",
    "        total_reward += reward_sum\n",
    "\n",
    "    env.close()\n",
    "    print(f\"ðŸŽ¥ Video saved in '{video_dir}/' | Reward: {total_reward:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ed1aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: expecting '}' (1053249624.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmodel_path=f\"{flappy_dqn.pt\",\u001b[39m\n                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m f-string: expecting '}'\n"
     ]
    }
   ],
   "source": [
    "record_trained_agent_video(\n",
    "    model_path=f\"{CHECKPOINTS_DIR}/flappy_dqn_{hashlib.md5(random.randbytes(64)).hexdigest()}.pt\",\n",
    "    video_dir=VIDEOS_DIR,\n",
    "    frame_skip=frame_skip,\n",
    "    device=device,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
