{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3291abbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefaniancu/Documents/Q-learning-CNN-in-Flappy-Bird-Environment/.venv/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets # for Mist\n",
    "import torchvision.transforms as transforms # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn # To inherit our neural network\n",
    "from torch.utils.data import DataLoader # For management of the dataset (batches)\n",
    "from tqdm import tqdm # For nice progress bar!\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import flappy_bird_gymnasium as flappy_bird\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from preprocessing.preprocessing import preprocess_frame\n",
    "from PIL import Image\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from static_variables import CHECKPOINTS_DIR, VIDEOS_DIR, PHOTOS_DIR, make_dirs\n",
    "import os\n",
    "import hashlib\n",
    "from preprocessing.preprocessing import preprocess_frame\n",
    "from model import DQN_CNN\n",
    "from save_model.utils import save_model, load_model, record_trained_agent_video\n",
    "from replay_buffer.ReplayBuffer import ReplayBuffer\n",
    "from model import save_input_frames\n",
    "from save_model.utils import transition\n",
    "import time\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367732a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe492045",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = torch.device(device)\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "frame_skip = 4\n",
    "train_every = 4   \n",
    "\n",
    "# Training\n",
    "data_type = torch.float32\n",
    "batch_size = 64\n",
    "T_iterations = 1_000_000\n",
    "initial_exploration = 15_000\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer_size = 150_000\n",
    "\n",
    "# DQN\n",
    "alpha = 1e-4\n",
    "gamma = 0.99\n",
    "\n",
    "# Target network (soft update)\n",
    "target_network_incorporation_rate = 0.005\n",
    "\n",
    "# Epsilon-greedy\n",
    "e_start = 0.1\n",
    "e_end = 0.0001\n",
    "k_epsilon = -1 / T_iterations * np.log(e_end / e_start)\n",
    "decay_factor = np.exp(-k_epsilon)\n",
    "\n",
    "\n",
    "\n",
    "# Early stopping parameters\n",
    "avg_window = 100          # moving average window\n",
    "patience = 2000            # episodes to wait without improvement\n",
    "min_delta = 0.5           # minimum improvement to count as progress\n",
    "\n",
    "reward_window = deque(maxlen=avg_window)\n",
    "best_avg_reward = -float(\"inf\")\n",
    "no_improve_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "policy_net = DQN_CNN(frame_skip).to(device)\n",
    "target_net = DQN_CNN(frame_skip).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "memory = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "global_step = 0\n",
    "transition_steps = 0\n",
    "frame_id = 0\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# timing trackers\n",
    "training_start_time = time.perf_counter()\n",
    "episode_times = deque(maxlen=100)\n",
    "\n",
    "\n",
    "for episode in range(T_iterations):\n",
    "    episode_start_time = time.perf_counter()\n",
    "    env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    current_state, _, _ = transition(0, env, frame_skip)\n",
    "    # save the frames from current_state for debug\n",
    "    \n",
    "\n",
    "    while not done:\n",
    "        global_step += 1\n",
    "\n",
    "        epsilon = e_end + (e_start - e_end) * np.exp(-k_epsilon * transition_steps)\n",
    "\n",
    "        state_tensor = torch.tensor(\n",
    "            current_state, dtype=torch.float32, device=device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state_tensor).argmax(dim=1).item()\n",
    "                # if global_step % 100 == 0:\n",
    "                #     save_input_frames(\n",
    "                #         state_tensor.cpu(),\n",
    "                #         f\"vis/step_{global_step:06d}_input.png\"\n",
    "                #     )\n",
    "\n",
    "        next_state, reward, done = transition(action, env, frame_skip)\n",
    "\n",
    "        memory.push(current_state, action, reward, next_state, done)\n",
    "        episode_reward += reward\n",
    "        current_state = next_state\n",
    "        transition_steps += 1\n",
    "\n",
    "        if len(memory) >= batch_size and global_step >= initial_exploration and global_step % train_every == 0:\n",
    "            states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Action selection using online (policy) network\n",
    "                next_actions = policy_net(next_states).argmax(1, keepdim=True)\n",
    "\n",
    "                # Action evaluation using target network\n",
    "                next_q = target_net(next_states).gather(1, next_actions)\n",
    "\n",
    "                target_q = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "\n",
    "            loss = criterion(q_values, target_q)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for t_param, p_param in zip(\n",
    "                    target_net.parameters(), policy_net.parameters()\n",
    "                ):\n",
    "                    t_param.data.mul_(1.0 - target_network_incorporation_rate)\n",
    "                    t_param.data.add_(target_network_incorporation_rate * p_param.data)\n",
    "\n",
    "    # ---------- episode end ----------\n",
    "    reward_window.append(episode_reward)\n",
    "\n",
    "    if len(reward_window) == avg_window:\n",
    "        avg_reward = np.mean(reward_window)\n",
    "\n",
    "        if avg_reward > best_avg_reward + min_delta:\n",
    "            best_avg_reward = avg_reward\n",
    "            no_improve_counter = 0\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save(policy_net.state_dict(), \"best_dqn.pt\")\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "\n",
    "        if no_improve_counter >= patience and epsilon < 0.05:\n",
    "            print(\n",
    "                f\"\\nðŸ›‘ Early stopping!\\n\"\n",
    "                f\"Best Avg Reward: {best_avg_reward:.2f}\\n\"\n",
    "                f\"Episode: {episode}\"\n",
    "                f\"Steps: {transition_steps}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    best_reward = max(best_reward, episode_reward)\n",
    "\n",
    "        # ---------- timing ----------\n",
    "    episode_time = time.perf_counter() - episode_start_time\n",
    "    episode_times.append(episode_time)\n",
    "\n",
    "    total_training_time = time.perf_counter() - training_start_time\n",
    "    avg_last_100_time = np.mean(episode_times)\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"Episode {episode:5d} | \"\n",
    "        f\"Reward: {episode_reward:7.2f} | \"\n",
    "        f\"Avg({avg_window}): {np.mean(reward_window):7.2f} | \"\n",
    "        f\"Epsilon: {epsilon:.4f} | \"\n",
    "        f\"Best Avg Reward: {best_avg_reward:7.2f} | \"\n",
    "        f\"Best Reward: {best_reward:7.2f} | \"\n",
    "        f\"Steps: {transition_steps:7d} | \"\n",
    "        f\"Ep Time: {episode_time:6.2f}s | \"\n",
    "        f\"Avg100 Time: {avg_last_100_time:6.2f}s | \"\n",
    "        f\"Total Time: {total_training_time/60:7.2f}m\"\n",
    "    )\n",
    "\n",
    "env.close()\n",
    "\n",
    "total_time = time.time() - training_start_time\n",
    "print(\"\\n====== Training Time Summary ======\")\n",
    "print(f\"Total training time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Average episode time (last 100): {np.mean(episode_times):.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5478de",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = save_model(\n",
    "    policy_net,\n",
    "    target_net,\n",
    "    optimizer,\n",
    "    dir=CHECKPOINTS_DIR,\n",
    "    global_step=global_step,\n",
    "    best_reward=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ed1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_trained_agent_video(\n",
    "    model_path=path,\n",
    "    video_dir=VIDEOS_DIR,\n",
    "    frame_skip=frame_skip,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
