{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3291abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets # for Mist\n",
    "import torchvision.transforms as transforms # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn # To inherit our neural network\n",
    "from torch.utils.data import DataLoader # For management of the dataset (batches)\n",
    "from tqdm import tqdm # For nice progress bar!\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import flappy_bird_gymnasium as flappy_bird\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from preprocessing.preprocessing import preprocess_frame\n",
    "from PIL import Image\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from static_variables import CHECKPOINTS_DIR, VIDEOS_DIR, PHOTOS_DIR, make_dirs\n",
    "import os\n",
    "import hashlib\n",
    "from preprocessing.preprocessing import preprocess_frame\n",
    "from model import DQN_CNN\n",
    "from save_model.utils import save_model, load_model, record_trained_agent_video\n",
    "from replay_buffer.ReplayBuffer import ReplayBuffer\n",
    "from model import save_input_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "367732a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe492045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = torch.device(device)\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c40f957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "frame_skip = 4\n",
    "\n",
    "# Training\n",
    "data_type = torch.float32\n",
    "batch_size = 32\n",
    "T_iterations = 1000   # very important\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer_size = 5000\n",
    "# DQN\n",
    "alpha = 1e-4\n",
    "gamma = 0.99\n",
    "\n",
    "# Target network (soft update)\n",
    "target_network_incorporation_rate = 0.005\n",
    "\n",
    "# Epsilon-greedy\n",
    "e_start = 1.0\n",
    "e_end = 0.05\n",
    "k_epsilon = -1 / T_iterations * np.log(e_end / e_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ada78b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(action, env, frame_skip):\n",
    "    global frame_id\n",
    "\n",
    "    frame_stack = []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    _, reward, terminated, truncated, _ = env.step(action)\n",
    "    frame = preprocess_frame(env.render())\n",
    "    frame_stack.append(frame)\n",
    "\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "    for _ in range(frame_skip - 1):\n",
    "        if done:\n",
    "            frame_stack.append(frame)  # pad with last frame\n",
    "            continue\n",
    "\n",
    "        _, reward, terminated, truncated, _ = env.step(0)\n",
    "        frame = preprocess_frame(env.render())\n",
    "        frame_stack.append(frame)\n",
    "\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Image.fromarray(frame).save(\n",
    "        #     f\"{PHOTOS_DIR}/frame_{'died_' if done else ''}{frame_id:05d}.png\"\n",
    "        # )\n",
    "        #frame_id += 1\n",
    "\n",
    "    return np.stack(frame_stack), total_reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "532e1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefaniancu/Documents/Proiecte/Q-Learning CNN Flappy Bird/.venv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/stefaniancu/Documents/Proiecte/Q-Learning CNN Flappy Bird/.venv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Reward: -1.30 | Epsilon: 0.9665 | Best Reward: -1.30\n",
      "Episode 1 | Reward: -1.30 | Epsilon: 0.9341 | Best Reward: -1.30\n",
      "Episode 2 | Reward: -1.30 | Epsilon: 0.9029 | Best Reward: -1.30\n",
      "Episode 3 | Reward: -2.50 | Epsilon: 0.8728 | Best Reward: -1.30\n",
      "Episode 4 | Reward: -1.20 | Epsilon: 0.8390 | Best Reward: -1.20\n",
      "Episode 5 | Reward: 1.70 | Epsilon: 0.8111 | Best Reward: 1.70\n",
      "Episode 6 | Reward: -0.50 | Epsilon: 0.7798 | Best Reward: 1.70\n",
      "Episode 7 | Reward: -1.30 | Epsilon: 0.7541 | Best Reward: 1.70\n",
      "Episode 8 | Reward: -1.30 | Epsilon: 0.7292 | Best Reward: 1.70\n",
      "Episode 9 | Reward: -3.70 | Epsilon: 0.7052 | Best Reward: 1.70\n",
      "Episode 10 | Reward: -1.30 | Epsilon: 0.6821 | Best Reward: 1.70\n",
      "Episode 11 | Reward: -1.30 | Epsilon: 0.6598 | Best Reward: 1.70\n",
      "Episode 12 | Reward: -1.30 | Epsilon: 0.6382 | Best Reward: 1.70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#print(\"Current state: \", current_state.shape, type(current_state), len(current_state))\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ---------- epsilon decay ----------\u001b[39;00m\n\u001b[32m     28\u001b[39m epsilon = e_end + (e_start - e_end) * np.exp(-k_epsilon * global_step)\n\u001b[32m     30\u001b[39m state_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m     \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#print(\"State tensor shape: \", state_tensor.shape)\u001b[39;00m\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# ---------- epsilon-greedy ----------\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m random.random() < epsilon:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "policy_net = DQN_CNN(frame_skip).to(device)\n",
    "target_net = DQN_CNN(frame_skip).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "memory = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "global_step = 0\n",
    "frame_id = 0\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "for episode in range(T_iterations):\n",
    "    env.reset()\n",
    "\n",
    "    # ---------- initialize frame stack ----------\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    current_state, _, _ = transition(0, env, frame_skip)\n",
    "\n",
    "    while not done:\n",
    "        global_step += 1\n",
    "        #print(\"Current state: \", current_state.shape, type(current_state), len(current_state))\n",
    "        # ---------- epsilon decay ----------\n",
    "        epsilon = e_end + (e_start - e_end) * np.exp(-k_epsilon * global_step)\n",
    "        \n",
    "        state_tensor = torch.tensor(\n",
    "             current_state, dtype=torch.float32\n",
    "        ).unsqueeze(0).to(device)\n",
    "        #print(\"State tensor shape: \", state_tensor.shape)\n",
    "\n",
    "        # ---------- epsilon-greedy ----------\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # if global_step % 100 == 0:\n",
    "                #     save_input_frames(\n",
    "                #         state_tensor.cpu(),\n",
    "                #         f\"vis/step_{global_step:06d}_input.png\"\n",
    "                #     )\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "\n",
    "        next_state, reward, done = transition(action, env, frame_skip)\n",
    "        memory.push(current_state, action, reward, next_state, done)\n",
    "\n",
    "        episode_reward += reward\n",
    "        current_state = next_state\n",
    "\n",
    "        # ---------- learning ----------\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(next_states).max(1, keepdim=True)[0]\n",
    "                target_q = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "            loss = nn.MSELoss()(q_values, target_q)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ---------- soft target update ----------\n",
    "            for target_param, policy_param in zip(\n",
    "                target_net.parameters(), policy_net.parameters()\n",
    "            ):\n",
    "                target_param.data.copy_(\n",
    "                    target_network_incorporation_rate * policy_param.data\n",
    "                    + (1.0 - target_network_incorporation_rate) * target_param.data\n",
    "                )\n",
    "                \n",
    "    best_reward = max(best_reward, episode_reward)\n",
    "    print(\n",
    "        f\"Episode {episode} | Reward: {episode_reward:.2f} | Epsilon: {epsilon:.4f} | Best Reward: {best_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5478de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to ../checkpoints/1c3c894cb256d7202025c9600dd952e0\n"
     ]
    }
   ],
   "source": [
    "path = save_model(\n",
    "    policy_net,\n",
    "    target_net,\n",
    "    optimizer,\n",
    "    dir=CHECKPOINTS_DIR,\n",
    "    global_step=global_step,\n",
    "    best_reward=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ed1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ Video saved in '../videos/' | Reward: 15.40\n"
     ]
    }
   ],
   "source": [
    "record_trained_agent_video(\n",
    "    model_path=path,\n",
    "    video_dir=VIDEOS_DIR,\n",
    "    frame_skip=frame_skip,\n",
    "    device=device,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
